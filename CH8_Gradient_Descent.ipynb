{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial, reduce\n",
    "import re, math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# functions for working with vectors\n",
    "#\n",
    "\n",
    "def vector_add(v, w):\n",
    "    \"\"\"adds two vectors componentwise\"\"\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    return reduce(vector_add, vectors)\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"compute the vector whose i-th element is the mean of the\n",
    "    i-th elements of the input vectors\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "\n",
    "def squared_distance(v, w):\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "   return math.sqrt(squared_distance(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 梯度下降思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 估算梯度   \n",
    "差商估算法：直接使用微分的定義暴力估算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6f67da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def plot_estimated_derivative():\n",
    "\n",
    "    def square(x):\n",
    "        return x * x\n",
    "\n",
    "    def derivative(x):\n",
    "        return 2 * x\n",
    "\n",
    "    derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
    "\n",
    "    # plot to show they're basically the same\n",
    "    import matplotlib.pyplot as plt\n",
    "    x = range(-10,10)\n",
    "    plt.plot(x, list(map(derivative, x)), 'rx')           # red  x\n",
    "    plt.plot(x, list(map(derivative_estimate, x)), 'b+')  # blue +\n",
    "    plt.show()                                      # purple *, hopefully\n",
    "\n",
    "plot_estimated_derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "\n",
    "    # add h to just the i-th element of v\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 使用梯度\n",
    "這個範例很奇怪，沒有使用微分，就是直接拿結果值硬幹，換句話說步長是錯的   \n",
    "阿，我誤會了，我們算的是平方，他已過一次了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3601098330270884e-06, 1.3601098330270884e-06, 4.533699443423632e-06]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"\n",
    "    move step_size in the direction from v\n",
    "    搞笑，方向應該是逆梯度，但不包含在這個函數，在 step_size 的參數中加上負值\n",
    "    \"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "# 對平方作微分\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v) \n",
    "    next_v = step(v, gradient, -0.01) #搞笑，步長是負的參數放在這裡\n",
    "    if distance(next_v, v) < tolerance:\n",
    "        break\n",
    "    v = next_v\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8.4 選擇正確步長\n",
    "選擇步長是藝術，這裡建議每一步使用最小化目標函數來達到   \n",
    "但是有些步長可能會導致錯誤，所以要加一個safe函數，如下  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 綜合\n",
    "這個範例很奇怪，沒有使用微分，就是直接拿結果值硬幹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn) # 注意 key 的妙用\n",
    "        next_value = target_fn(next_theta)\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using minimize_batch\n",
      "minimum v [0.001063382396627933, 0.00031901471898837996, -0.001063382396627933]\n",
      "minimum value 2.3633346338475687e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"using minimize_batch\")\n",
    "\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 隨機梯度下降法\n",
    "有點看不太懂 stochastic 與 batch的差別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes[:len(data) // 2]:         # return the data in that order @ 只取部分樣本\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書後面關於線性回歸的範例  \n",
    "到目前為止看起來 batch 與 stochastic 的版本差別是差在\n",
    "batch 算過所有樣本點才 iterate 一次，而 stochastic 每次只 iterate 一個樣本點\n",
    "不過還是有點奇怪，那個 for 迴圈中每個樣本點跑一次後才結束，那是不是random有差嗎@@   \n",
    "或每次只interate一個樣本，順序會對結果產生差異  \n",
    "而且在別的文章看來，或許不用所有樣本點都取到也可以算出來  \n",
    "所以經過實驗，random中只取部分樣本也是可以算出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(alpha, beta, x_i):\n",
    "    return beta * x_i + alpha\n",
    "\n",
    "def error(alpha, beta, x_i, y_i):\n",
    "    return y_i - predict(alpha, beta, x_i)\n",
    "\n",
    "def sum_of_squared_errors(alpha, beta, x, y):\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2\n",
    "               for x_i, y_i in zip(x, y))\n",
    "\n",
    "def least_squares_fit(x,y):\n",
    "    \"\"\"given training values for x and y,\n",
    "    find the least-squares values of alpha and beta\"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta\n",
    "\n",
    "def total_sum_of_squares(y):\n",
    "    \"\"\"the total squared variation of y_i's from their mean\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha, beta, x, y):\n",
    "    \"\"\"the fraction of variation in y captured by the model, which equals\n",
    "    1 - the fraction of variation in y not captured by the model\"\"\"\n",
    "\n",
    "    return 1.0 - (sum_of_squared_errors(alpha, beta, x, y) /\n",
    "                  total_sum_of_squares(y))\n",
    "\n",
    "def squared_error(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return error(alpha, beta, x_i, y_i) ** 2\n",
    "\n",
    "def squared_error_gradient(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return [-2 * error(alpha, beta, x_i, y_i),       # alpha partial derivative\n",
    "            -2 * error(alpha, beta, x_i, y_i) * x_i] # beta partial derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 22.94755241346903\n",
      "beta 0.903865945605865\n",
      "r-squared 0.3291078377836305\n",
      "\n",
      "gradient descent:\n",
      "alpha 22.64747007529572\n",
      "beta 0.9263290359289186\n"
     ]
    }
   ],
   "source": [
    "from stats import mean, correlation, standard_deviation, de_mean\n",
    "\n",
    "num_friends_good = [49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "daily_minutes_good = [68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "\n",
    "alpha, beta = least_squares_fit(num_friends_good, daily_minutes_good)\n",
    "print(\"alpha\", alpha)\n",
    "print(\"beta\", beta)\n",
    "\n",
    "print(\"r-squared\", r_squared(alpha, beta, num_friends_good, daily_minutes_good))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"gradient descent:\")\n",
    "# choose random value to start\n",
    "random.seed(0)\n",
    "theta = [random.random(), random.random()]\n",
    "alpha, beta = minimize_stochastic(squared_error,\n",
    "                                  squared_error_gradient,\n",
    "                                  num_friends_good,\n",
    "                                  daily_minutes_good,\n",
    "                                  theta,\n",
    "                                  0.0001)\n",
    "print(\"alpha\", alpha)\n",
    "print(\"beta\", beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha 22.93746417548679\n",
    "beta 0.9043371597664965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    " \n",
    "def in_random_order_2(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes[:len(data) // 2]:                          # return the data in that order\n",
    "        yield data[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1, 0, 4, 3, 6, 7, 9, 2, 5]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(in_random_order([1,2,3,4,5,6,7,8,9,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 9, 4, 2]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(in_random_order_2([1,2,3,4,5,6,7,8,9,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py34]",
   "language": "python",
   "name": "Python [py34]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
